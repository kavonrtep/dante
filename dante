#!/usr/bin/env python
"""
This is wrapper for dante.py script
it will find the path to database files
"""
import argparse
import copy
import os
import re
import shutil
import subprocess
import tempfile
from itertools import cycle
from multiprocessing import Pool
from version import __version__

master_tmp = tempfile.mkdtemp()
tempfile.tempdir = master_tmp
print("Temporary directory: ", master_tmp)

import dante

def extract_best_hit_components(s: str):
    """
    Given a string like
      Helitron-HEL2__REXdb_ID17615|Class_II|Subclass_2|Helitron:4632-4733[59percent]
    returns a tuple:
      (prefix, start_int, end_int, bracketed_str)

    where `prefix` is everything up to the last colon,
    `start_int` and `end_int` are the two numbers,
    and `bracketed_str` is the '[…]' part.
    """
    # split on the last ':' so we don’t get tripped up by colons earlier in the prefix
    try:
        prefix, tail = s.rsplit(':', 1)
    except ValueError:
        raise ValueError("String must contain at least one ':' before the coordinates.")

    # tail should look like "4632-4733[59percent]"
    m = re.fullmatch(r'(\d+)-(\d+)\[(.+)\]', tail)
    if not m:
        raise ValueError(f"Unexpected format for coordinates/bracket: {tail!r}")

    start, end, inner = m.groups()
    return prefix, int(start), int(end), f'[{inner}]'

class Gff3Feature:
    """
    Class for gff3 feature
    """

    def __init__(self, line):
        self.line = line
        self.items = line.strip().split('\t')
        self.header = self.items[0]
        self.source = self.items[1]
        self.type = self.items[2]
        self.start = int(self.items[3])
        self.end = int(self.items[4])
        self.score = self.items[5]
        self.strand = self.items[6]
        self.frame = self.items[7]
        self.attributes = self.items[8]
        self.attributes_dict = {}
        for item in self.attributes.split(';'):
            if item != '':
                key, value = item.split('=')
                self.attributes_dict[key] = value

        self.attributes_str = ';'.join(
            ['{}={}'.format(key, value) for key, value in self.attributes_dict.items()]
            )

    def __str__(self):
        return '\t'.join(
            [self.header, self.source, self.type, str(self.start), str(self.end),
             self.score, self.strand, self.frame, self.attributes_str]
            ) + '\n'

    def __repr__(self):
        return '\t'.join(
            [self.header, self.source, self.type, str(self.start), str(self.end),
             self.score, self.strand, self.frame, self.attributes_str]
            ) + '\n'

    def __eq__(self, other):
        return self.line_recalculated() == other.line_recalculated()

    def __hash__(self):
        return hash(self.line_recalculated())

    def get_line(self):
        """returns original line"""
        return self.line

    def overlap(self, other):
        """
        Check if two features overlap
        :param other:
        :return:
        """
        if self.start <= other.end and self.end >= other.start:
            return True
        else:
            return False

    def line_recalculated(self):
        """
        :return:
        string with recalculated line
        """
        return '\t'.join(
            [self.header, self.source, self.type, str(self.start), str(self.end),
             self.score, self.strand, self.frame, self.attributes_str]
            ) + '\n'

    def __lt__(self, other):
        width = self.end - self.start
        other_width = other.end - other.start
        return width < other_width

    def __gt__(self, other):
        width = self.end - self.start
        other_width = other.end - other.start
        return width > other_width

    def identical_region(self, other):
        """
        Check if two features are identical
        :param other:
        :return:
        """
        if self.start == other.start and self.end == other.end and self.header == \
                other.header:
            return True
        else:
            return False
class Gff3Feature:
    """
    Class for GFF3 feature
    """

    def __init__(self, line):
        self.line = line
        self.items = line.strip().split('\t')
        self.header = self.items[0]
        self.source = self.items[1]
        self.type = self.items[2]
        self.start = int(self.items[3])
        self.end = int(self.items[4])
        self.score = self.items[5]
        self.strand = self.items[6]
        self.frame = self.items[7]
        self.attributes = self.items[8]

        # parse attributes into dict, recording original order
        self.attributes_dict = {}
        self.attribute_order = []
        for item in self.attributes.split(';'):
            if not item:
                continue
            key, value = item.split('=', 1)
            self.attributes_dict[key] = value
            self.attribute_order.append(key)

    def _format_attributes(self):
        """
        Return a semicolon-joined string of attributes in the original order,
        pulling current values from attributes_dict.
        """
        return ';'.join(f"{key}={self.attributes_dict.get(key,'')}"
                        for key in self.attribute_order)

    def __str__(self):
        attrs = self._format_attributes()
        return '\t'.join([
            self.header,
            self.source,
            self.type,
            str(self.start),
            str(self.end),
            self.score,
            self.strand,
            self.frame,
            attrs
        ]) + '\n'

    def __repr__(self):
        return self.__str__()

    def __eq__(self, other):
        return self.line_recalculated() == other.line_recalculated()

    def __hash__(self):
        return hash(self.line_recalculated())

    def get_line(self):
        """Returns the original line (unmodified)."""
        return self.line

    def overlap(self, other):
        """
        Check if two features overlap.
        """
        return not (self.end < other.start or self.start > other.end)

    def line_recalculated(self):
        """
        Return a GFF-formatted line built from the current attributes_dict
        (so reflects any updates), preserving original attribute order.
        """
        return self.__str__()

    def __lt__(self, other):
        return (self.end - self.start) < (other.end - other.start)

    def __gt__(self, other):
        return (self.end - self.start) > (other.end - other.start)

    def identical_region(self, other):
        """
        Check if two features share the same seqid and coordinates.
        """
        return (self.start == other.start and
                self.end == other.end and
                self.header == other.header)



def read_fasta_sequence_size(fasta_file):
    """Read size of sequence into dictionary"""
    fasta_dict = {}
    with open(fasta_file, 'r') as f:
        for line in f:
            if line[0] == '>':
                # remove part of name after space or tab
                header = line.strip().split()[0][1:]
                fasta_dict[header] = 0
            else:
                fasta_dict[header] += len(line.strip())
    return fasta_dict

def read_single_fasta_to_dictionary(fh):
    """
    Read fasta file into dictionary
    :param fh:
    :return:
    fasta_dict
    """
    fasta_dict = {}
    for line in fh:
        if line[0] == '>':
            header = line.strip().split()[0][1:]  # remove part of name after space
            fasta_dict[header] = []
        else:
            fasta_dict[header] += [line.strip()]
    fasta_dict = {k: ''.join(v) for k, v in fasta_dict.items()}
    return fasta_dict

def split_fasta_by_length(fasta_file, length_threshold=1000000):
    """
    Split fasta file into two files based on sequence length threshold.
    Sequences shorter than threshold go to short file, longer ones to long file.
    This function processes the file line by line without loading into memory.

    :param fasta_file: input fasta file path
    :param length_threshold: length threshold to split sequences (default 1000000)
    :return: tuple of (short_file_path, long_file_path, has_short, has_long)
    """
    short_file = tempfile.NamedTemporaryFile(delete=False, mode='w', suffix='_short.fasta')
    long_file = tempfile.NamedTemporaryFile(delete=False, mode='w', suffix='_long.fasta')

    short_file_path = short_file.name
    long_file_path = long_file.name

    has_short = False
    has_long = False

    with open(fasta_file, 'r') as f:
        current_header = None
        current_seq_lines = []
        current_length = 0

        for line in f:
            if line.startswith('>'):
                # Process previous sequence if exists
                if current_header is not None:
                    if current_length < length_threshold:
                        short_file.write(current_header)
                        for seq_line in current_seq_lines:
                            short_file.write(seq_line)
                        has_short = True
                    else:
                        long_file.write(current_header)
                        for seq_line in current_seq_lines:
                            long_file.write(seq_line)
                        has_long = True

                # Start new sequence
                current_header = line
                current_seq_lines = []
                current_length = 0
            else:
                current_seq_lines.append(line)
                current_length += len(line.strip())

        # Process last sequence
        if current_header is not None:
            if current_length < length_threshold:
                short_file.write(current_header)
                for seq_line in current_seq_lines:
                    short_file.write(seq_line)
                has_short = True
            else:
                long_file.write(current_header)
                for seq_line in current_seq_lines:
                    long_file.write(seq_line)
                has_long = True

    short_file.close()
    long_file.close()

    return short_file_path, long_file_path, has_short, has_long


def split_fasta_to_parts(fasta_file, number_of_parts):
    fasta_file_dir = tempfile.mkdtemp()
    # split fasta file to parts and save them to temporary directory
    # first count number of sequences
    with open(fasta_file, 'r') as f:
        number_of_sequences = 0
        for line in f:
            if line[0] == '>':
                number_of_sequences += 1

    # Adjust number of parts if there are fewer sequences than requested parts
    if number_of_sequences < number_of_parts:
        number_of_parts = number_of_sequences

    # Calculate part sizes
    part_size = int(number_of_sequences / number_of_parts)
    remainder = number_of_sequences % number_of_parts

    # Distribute sequences: first 'remainder' parts get one extra sequence
    part_sizes = [part_size + 1 if i < remainder else part_size
                  for i in range(number_of_parts)]

    part_files = [os.path.join(fasta_file_dir, F'part_{i}.fasta')
                  for i in range(number_of_parts)]
    print(part_files)
    # Split file in a single pass
    with open(fasta_file, 'r') as f:
        current_part = 0
        sequences_in_current_part = 0
        f_out = open(part_files[current_part], 'w')

        for line in f:
            if line[0] == '>':
                # Check if we need to move to the next part
                if (sequences_in_current_part >= part_sizes[current_part] and
                    current_part < number_of_parts - 1):
                    f_out.close()
                    current_part += 1
                    sequences_in_current_part = 0
                    f_out = open(part_files[current_part], 'w')
                sequences_in_current_part += 1
            f_out.write(line)

        f_out.close()

    return part_files




def split_fasta_to_chunks(fasta_file, chunk_size=100000000, overlap=100000):
    """
    Split fasta file to chunks, sequences longe than chuck size are split to overlaping
    peaces. If sequences are shorter, chunck with multiple sequences are created.
    :param fasta_file:

    :param fasta_file:
    :param chunk_size:
    :param overlap:
    :return:
    fasta_file_split
    matching_table
    """
    min_chunk_size = chunk_size * 2
    print("analyzing fasta file")
    fasta_dict = read_fasta_sequence_size(fasta_file)
    print("fasta file loaded")
    print("number of sequences: ", len(fasta_dict))
    # calculates ranges for splitting of fasta files and store them in list
    matching_table = []
    fasta_file_split = tempfile.NamedTemporaryFile(delete=False).name
    print("calculating ranges for splitting of fasta files")
    for header, size in fasta_dict.items():
        if size > min_chunk_size:
            number_of_chunks = int(size / chunk_size)
            adjusted_chunk_size = int(size / number_of_chunks)
            for i in range(number_of_chunks):
                start = i * adjusted_chunk_size
                end = ((i + 1) *
                       adjusted_chunk_size
                       + overlap) if i + 1 < number_of_chunks else size
                new_header = header + '_' + str(i)
                matching_table.append([header, i, start, end, new_header])
        else:
            new_header = header + '_0'
            matching_table.append([header, 0, 0, size, new_header])
    print("splitting fasta file")
    # read sequences from fasta files and split them to chunks according to matching table
    # open output and input files, use with statement to close files
    # TODO - avoid using dictionary - could be problem for large files!!
    fasta_dict = read_single_fasta_to_dictionary(open(fasta_file, 'r'))
    with open(fasta_file_split, 'w') as fh_out:
        for header in fasta_dict:
            print(header)
            matching_table_part = [x for x in matching_table if x[0] == header]
            for header2, i, start, end, new_header in matching_table_part:
                fh_out.write('>' + new_header + '\n')
                fh_out.write(fasta_dict[header][start:end] + '\n')
    return fasta_file_split, matching_table

def make_temp_files(number_of_files):
    """
    Make named temporary files, file will not be deleted upon exit!
    :param number_of_files:
    :return:
    filepaths
    """
    temp_files = []
    for i in range(number_of_files):
        temp_files.append(tempfile.NamedTemporaryFile(delete=False).name)
        os.remove(temp_files[-1])
    return temp_files


def get_new_header_and_coordinates(header, start, end, matching_table):
    """
    Get new header and coordinates for sequence
    :param header:
    :param start:
    :param end:
    :param matching_table:
    :return:
    new_header
    new_start
    new_end
    """
    matching_table_part = [x for x in matching_table if x[0] == header]
    new_coords = []
    for chunk in matching_table_part:
        if chunk[2] <= start < chunk[3]:
            new_header = chunk[4]
            new_start = start - chunk[2]
            new_end = end - chunk[2]
            new_sequence_length = chunk[3] - chunk[2]
            new_coords.append([new_header, new_start, new_end, new_sequence_length])
    return new_coords


def get_original_header_and_coordinates(new_header, new_start, new_end, matching_table):
    """
    Get original header and coordinates for sequence
    :param new_header:
    :param new_start:
    :param new_end:
    :param matching_table:
    :return:
    original_header
    original_start
    original_end
    """
    matching_table_part = [x for x in matching_table if x[4] == new_header]
    real_chunk_size = matching_table_part[0][3] - matching_table_part[0][2]
    ori_header = matching_table_part[0][0]
    start = matching_table_part[0][2]
    ori_start = new_start + start
    ori_end = new_end + start
    return ori_header, ori_start, ori_end, real_chunk_size


# recalculate gff3 coordinates, use gff3_feature class
def recalculate_gff3_coordinates(gff3_file, matching_table):
    """
    Recalculate gff3 coordinates, use gff3_feature class
    :param gff3_file:
    :param matching_table:
    :return:
    gff3_file_recalculated
    """
    gff3_file_recalculated = tempfile.NamedTemporaryFile(delete=False).name

    with open(gff3_file, 'r') as fh_in:
        with open(gff3_file_recalculated, 'w') as fh_out:
            for line in fh_in:
                if line[0] == '#':
                    fh_out.write(line)
                else:
                    feature = Gff3Feature(line)
                    new_coords = get_new_header_and_coordinates(
                        feature.header, feature.start, feature.end, matching_table
                        )
                    for new_header, new_start, new_end, sequence_length in new_coords:
                        if new_start >= 1 and new_end <= sequence_length:
                            feature.header = new_header
                            feature.start = new_start
                            feature.end = new_end
                            fh_out.write(str(feature))
    return gff3_file_recalculated


# recalculate gff3 back to original coordinates, use gff3_feature class
def recalculate_gff3_back_to_original_coordinates(gff3_file, matching_table,
                                                  chunk_size, overlap):
    """
    Recalculate gff3 back to original coordinates, use gff3_feature class
    :param gff3_file:
    :param matching_table:
    :return:
    gff3_file_recalculated
    """
    gff3_file_recalculated = tempfile.NamedTemporaryFile(delete=False).name
    with open(gff3_file, 'r') as fh_in:
        with open(gff3_file_recalculated, 'w') as fh_out:
            for line in fh_in:
                if line[0] == '#':
                    fh_out.write(line)
                else:
                    feature = Gff3Feature(line)

                    ori_header, ori_start, ori_end, \
                        real_chunk_size = get_original_header_and_coordinates(
                        feature.header, feature.start, feature.end, matching_table
                        )
                    # recalculate coordinates on Best_Hit coordinates
                    # Best_Hit coordinates can be missing!
                    if "Best_Hit" in feature.attributes_dict:
                        best_hit = feature.attributes_dict["Best_Hit"]
                        s1, start, end, s4  = extract_best_hit_components(best_hit)
                        _, hit_start, hit_end, _ = get_original_header_and_coordinates(
                            feature.header, start, end, matching_table
                            )
                        # update Best_Hit attribute
                        feature.attributes_dict["Best_Hit"] = \
                            F'{s1}:{hit_start}-{hit_end}{s4}'

                    # if feature is too close (less than 100 nt) to the end ends of
                    # chunk,skip it
                    if feature.start < 100 or feature.end > real_chunk_size - 100:
                        continue
                    feature.header = ori_header
                    feature.start = ori_start
                    feature.end = ori_end
                    fh_out.write(str(feature))
    return gff3_file_recalculated




class CustomFormatter(
    argparse.ArgumentDefaultsHelpFormatter, argparse.RawDescriptionHelpFormatter
    ):
    """This is custom formatter for argparse"""
    pass

def add_version_to_gff3(gff3_file, version_info, REXdb_version):
    """
    Add version to gff3 file
    :param gff3_file:
    :param version_info:
    :return:
    """
    tmp_filename = os.path.join(gff3_file + '.tmp')
    with open(gff3_file, 'r') as src_file, open(tmp_filename, 'w') as tmp_file:
        line = src_file.readline()
        tmp_file.write(line)
        tmp_file.write("##DANTE version " + version_info + '\n')
        tmp_file.write("##REXdb version " + REXdb_version + '\n')
        shutil.copyfileobj(src_file, tmp_file)
        # Replace the original file with the new file
    os.replace(tmp_file.name, gff3_file)



parser = argparse.ArgumentParser(
    description='''Script performs similarity search on given DNA sequence(s) in (
    multi)fasta against our protein domains database of all Transposable element for 
    certain group of organisms (Viridiplantae or Metazoans). Domains are subsequently 
    annotated and classified - in case certain domain has multiple annotations 
    assigned, classification is derived from the common classification level of all of 
    them. Domains search is accomplished engaging LASTAL alignment tool.



    ''', epilog="""
    Extra database format:
    Extra database is FASTA file with protein domains sequences. This file is appended 
    to selected REXdb database. Header of sequences must contain information about 
    classification compatible with REXdb classification system and also protein domain 
    type. Example of FASTA header:
    
       >MNCI01000001.1:152848-153282 RH Class_I|LTR|Ty3/gypsy|non-chromovirus|OTA|Tat
       CQEALDNIMRELAQVSTVYSSQNDKSFYIYLTISDisissllcQKLDDGVELsvyylsha
       litYET*YIEVEKFFLALVVSFKK*rnylfrshINVICKDKVLRDITTNIYKNSRIA**K
       DILDEFGfhyisqa*TKGQVIATQLT
    
    where:
    
    MNCI01000001.1:152848-153282 is unique identifier of sequence in database.
    
    RH is type of protein domain.
    
    Class_I|LTR|Ty3/gypsy|non-chromovirus|OTA|Tat is classification of protein domain.
    
    """, formatter_class=CustomFormatter
    )

requiredNamed = parser.add_argument_group('required named arguments')
requiredNamed.add_argument(
    "-q", "--query", type=str, required=True,
    help='input DNA sequence to search for protein domains in a fasta format. '
         'Multifasta format allowed.'
    )
parser.add_argument('-D', '--database', type=str, required=False,
                    default='Viridiplantae_v4.0',
                    choices=[
                        'Viridiplantae_v4.0',
                        'Viridiplantae_v3.0',
                        'Metazoa_v3.1',
                        'Viridiplantae_v2.2',
                        'Metazoa_v3.0'],
                    help='Select version of RExDB database to use for similarity search'
                    )


parser.add_argument(
    "-o", "--domain_gff", type=str, help="output domains gff format",required=True
    )
parser.add_argument(
    "-dir", "--output_dir", type=str,
    help="specify if you want to change the output directory",
    default='.'
    )
parser.add_argument(
    "-M", "--scoring_matrix", type=str, default="BL80", choices=['BL80', 'BL62', 'MIQS'],
    help="specify scoring matrix to use for similarity search (BL80, BL62, MIQS)"
    )
parser.add_argument(
    "-thsc", "--threshold_score", type=int, default=80,
    help="percentage of the best score in the cluster to be tolerated when assigning "
         "annotations per base"
    )
parser.add_argument(
    "-wd", "--win_dom", type=int, default=10000000,
    help="window to process large input sequences sequentially"
    )
parser.add_argument(
    "-od", "--overlap_dom", type=int, default=10000,
    help="overlap of sequences in two consecutive windows"
    )
parser.add_argument(
    "-c", "--cpu", type=int, default=1,
    help="number of threads to use"
    )
parser.add_argument('-S', '--short_reads', action='store_true',
                    help='(DEPRECATED - now automatic) use this option if your input sequences are short reads',
                    default=False
                    )


parser.add_argument(
        '-e', '--extra_database', type=str, required=False,
        default=None,help='extra database to use for similarity search')

parser.add_argument('-v', '--version', action='version', version='%(prog)s ' + __version__)


args = parser.parse_args()
print(args)
script_path = os.path.dirname(os.path.realpath(__file__))
# add path to database files

pdb = F'{script_path}/tool-data/protein_domains/{args.database}_pdb'
cls = F'{script_path}/tool-data/protein_domains/{args.database}_class'


if args.extra_database is not None:
    # add extra database to default database
    tmpdir_obj = tempfile.TemporaryDirectory()
    db_tmp_dir = tmpdir_obj.name
    os.makedirs(db_tmp_dir, exist_ok=True)
    # TODO - unlink directory at the end
    classification_table = set()
    extra_classification_file = os.path.join(db_tmp_dir, 'extra.class')
    extra_fasta_file = os.path.join(db_tmp_dir, 'extra.fasta')
    with open(args.extra_database, 'r') as f, open(extra_fasta_file, 'w') as out:
        for line in f:
            if line[0] == ">":
                ## modify header
                name, domain, classification = line.split(" ")
                name_clean=name[1:].replace("-","_")
                new_header = ">NA-{}__{}\n".format(domain, name_clean)
                classification_string = "\t".join(classification.split("|"))
                classification_table.add("{}\t{}".format(name_clean, classification_string))
                out.write(new_header)
            else:
                out.write(line)
    with open(extra_classification_file, 'w') as f:
        f.writelines(classification_table)
    # concatenate extra database with pdb and cls to temporary files
    pdb_tmp = os.path.join(db_tmp_dir, 'pdb.fasta')
    cls_tmp = os.path.join(db_tmp_dir, 'cls.class')
    # concatenate files
    with open(pdb_tmp, 'w') as f:
        for file in [pdb, extra_fasta_file]:
            with open(file, 'r') as f_tmp:
                for line in f_tmp:
                    f.write(line)
    with open(cls_tmp, 'w') as f:
        for file in [cls, extra_classification_file]:
            with open(file, 'r') as f_tmp:
                for line in f_tmp:
                    f.write(line)
    # format lastdb
    cmd = ['lastdb', '-p', pdb_tmp,  pdb_tmp]
    subprocess.run(cmd)
    pdb = pdb_tmp
    cls = cls_tmp


args.protein_database = pdb
args.classification = cls
args.new_ldb = False
# run dante
chunk_size = 500000
overlap = 2000
length_threshold = 1000000

# Split input file into short and long sequences automatically
print("Analyzing input sequences and splitting by length...")
short_file, long_file, has_short, has_long = split_fasta_by_length(
    args.query, length_threshold=length_threshold
)

print(f"Sequences < {length_threshold} bp: {'Yes' if has_short else 'No'}")
print(f"Sequences >= {length_threshold} bp: {'Yes' if has_long else 'No'}")

# List to collect all output GFF files
all_output_gffs = []
files_to_cleanup = []

# Process short sequences (if any) - use short-read mode
if has_short:
    print("\n=== Processing short sequences ===")
    files_to_cleanup.append(short_file)

    if args.cpu == 1:
        print("Running DANTE on short sequences with 1 CPU")
        args_short = copy.deepcopy(args)
        args_short.query = short_file
        short_output = tempfile.NamedTemporaryFile(delete=False, suffix='_short.gff').name
        args_short.domain_gff = short_output
        args_short.short_reads = True
        dante.main(args_short)
        all_output_gffs.append(short_output)
    else:
        print(f"Splitting short sequences into {args.cpu} parts for parallel processing")
        parts = split_fasta_to_parts(short_file, number_of_parts=args.cpu)
        args_list = []
        output_files = make_temp_files(min(args.cpu, len(parts)))
        for i, part in enumerate(parts):
            args_part = copy.deepcopy(args)
            args_list.append(args_part)
            args_part.query = part
            args_part.domain_gff = output_files[i]
            args_part.short_reads = True

        with Pool(args.cpu) as p:
            p.map(dante.main, args_list)

        # Concatenate short sequence results
        short_combined = tempfile.NamedTemporaryFile(delete=False, suffix='_short_combined.gff').name
        with open(short_combined, 'w') as f:
            for output_file in output_files:
                with open(output_file, 'r') as f_tmp:
                    for line in f_tmp:
                        f.write(line)
                os.remove(output_file)

        all_output_gffs.append(short_combined)
        # Clean up parts directory
        shutil.rmtree(os.path.dirname(parts[0]))
else:
    print("No short sequences to process")
    os.remove(short_file)

# Process long sequences (if any) - use chunking mode
if has_long:
    print("\n=== Processing long sequences ===")
    files_to_cleanup.append(long_file)

    # Split fasta file to chunks
    fasta_file_split, matching_table = split_fasta_to_chunks(long_file, chunk_size, overlap)
    print('Getting fasta sequence sizes')
    fasta_seq_size = read_fasta_sequence_size(fasta_file_split)
    total_size = sum(fasta_seq_size.values())
    number_of_sequences = len(fasta_seq_size)
    print(f'Total size: {total_size}')
    print(f'Number of sequences: {number_of_sequences}')

    if total_size > chunk_size and number_of_sequences > 1 and args.cpu > 1:
        print("Running DANTE on chunks")
        seq_id_size_sorted = [i[0] for i in sorted(
            fasta_seq_size.items(), key=lambda x: int(x[1]), reverse=True
        )]
        number_of_temp_files = int(total_size / chunk_size) + 1
        if number_of_temp_files > number_of_sequences:
            number_of_temp_files = number_of_sequences

        temp_files_fasta = make_temp_files(number_of_temp_files)
        seq_id_file_path_dict = dict(zip(seq_id_size_sorted, cycle(temp_files_fasta)))

        # Write sequences to temporary files
        with open(fasta_file_split, 'r') as f:
            for line in f:
                if line[0] == '>':
                    header = line.strip().split(' ')[0][1:]
                    with open(seq_id_file_path_dict[header], 'a') as fout:
                        fout.write(line)
                else:
                    with open(seq_id_file_path_dict[header], 'a') as fout:
                        fout.write(line)
        os.remove(fasta_file_split)

        args_list = []
        output_files = make_temp_files(number_of_temp_files)
        for i, temp_file_out in enumerate(temp_files_fasta):
            args_part = copy.deepcopy(args)
            args_list.append(args_part)
            args_part.query = temp_files_fasta[i]
            args_part.domain_gff = output_files[i]
            args_part.short_reads = False

        print(f"Number of chunks: {number_of_temp_files}")
        with Pool(args.cpu) as p:
            p.map(dante.main, args_list, chunksize=1)

        for fn in temp_files_fasta:
            os.remove(fn)

        # Recalculate coordinates
        tmp_gff_unfiltered = tempfile.NamedTemporaryFile(delete=False).name
        with open(tmp_gff_unfiltered, 'w') as f:
            for args_part in args_list:
                tmp_gff = recalculate_gff3_back_to_original_coordinates(
                    args_part.domain_gff, matching_table, chunk_size, overlap
                )
                with open(tmp_gff, 'r') as f_tmp:
                    for line in f_tmp:
                        f.write(line)
                os.remove(tmp_gff)

        # Filter gff to remove duplicates
        gff3_header = "##gff-version 3\n"
        long_combined = tempfile.NamedTemporaryFile(delete=False, suffix='_long_combined.gff').name

        with open(long_combined, 'w') as f, open(tmp_gff_unfiltered, 'r') as f_tmp:
            p = subprocess.Popen(['sort', '-k1,1', '-k4,4n'], stdin=f_tmp,
                                stdout=subprocess.PIPE)
            last_line_coord = ""
            f.write(gff3_header)
            for line in p.stdout:
                if line.decode('utf-8').startswith('#'):
                    continue
                items = line.decode('utf-8').split('\t')
                coord = f'{items[0]}_{items[3]}_{items[4]}'
                if coord != last_line_coord:
                    f.write(line.decode('utf-8'))
                    last_line_coord = coord

        os.remove(tmp_gff_unfiltered)
        for args_part in args_list:
            os.remove(args_part.domain_gff)

        all_output_gffs.append(long_combined)
    else:
        print("Running DANTE on whole long sequence file")
        args_long = copy.deepcopy(args)
        args_long.query = long_file
        long_output = tempfile.NamedTemporaryFile(delete=False, suffix='_long.gff').name
        args_long.domain_gff = long_output
        args_long.short_reads = False
        dante.main(args_long)
        all_output_gffs.append(long_output)
else:
    print("No long sequences to process")
    os.remove(long_file)

# Merge all results into final output file
print("\n=== Merging results ===")
gff3_header = "##gff-version 3\n"

with open(args.domain_gff, 'w') as f_out:
    f_out.write(gff3_header)
    for gff_file in all_output_gffs:
        with open(gff_file, 'r') as f_in:
            for line in f_in:
                if not line.startswith('##gff-version'):
                    f_out.write(line)
        os.remove(gff_file)

# Clean up temporary split files
for fn in files_to_cleanup:
    if os.path.exists(fn):
        os.remove(fn)






# Add version to gff3 file
add_version_to_gff3(args.domain_gff, __version__, args.database)
# remove temporary files, recursive remove directory
if args.extra_database is not None:
    shutil.rmtree(db_tmp_dir)


shutil.rmtree(master_tmp)
