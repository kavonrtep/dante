#!/usr/bin/env python
"""
This is wrapper for dante.py script
it will find the path to database files
"""
import argparse
import copy
import os
import shutil
import subprocess
import tempfile
from itertools import cycle
from multiprocessing import Pool
from version import __version__

master_tmp = tempfile.mkdtemp()
tempfile.tempdir = master_tmp
print("Temporary directory: ", master_tmp)

import dante
class Gff3Feature:
    """
    Class for gff3 feature
    """

    def __init__(self, line):
        self.line = line
        self.items = line.strip().split('\t')
        self.header = self.items[0]
        self.source = self.items[1]
        self.type = self.items[2]
        self.start = int(self.items[3])
        self.end = int(self.items[4])
        self.score = self.items[5]
        self.strand = self.items[6]
        self.frame = self.items[7]
        self.attributes = self.items[8]
        self.attributes_dict = {}
        for item in self.attributes.split(';'):
            if item != '':
                key, value = item.split('=')
                self.attributes_dict[key] = value

        self.attributes_str = ';'.join(
            ['{}={}'.format(key, value) for key, value in self.attributes_dict.items()]
            )

    def __str__(self):
        return '\t'.join(
            [self.header, self.source, self.type, str(self.start), str(self.end),
             self.score, self.strand, self.frame, self.attributes_str]
            ) + '\n'

    def __repr__(self):
        return '\t'.join(
            [self.header, self.source, self.type, str(self.start), str(self.end),
             self.score, self.strand, self.frame, self.attributes_str]
            ) + '\n'

    def __eq__(self, other):
        return self.line_recalculated() == other.line_recalculated()

    def __hash__(self):
        return hash(self.line_recalculated())

    def get_line(self):
        """returns original line"""
        return self.line

    def overlap(self, other):
        """
        Check if two features overlap
        :param other:
        :return:
        """
        if self.start <= other.end and self.end >= other.start:
            return True
        else:
            return False

    def line_recalculated(self):
        """
        :return:
        string with recalculated line
        """
        return '\t'.join(
            [self.header, self.source, self.type, str(self.start), str(self.end),
             self.score, self.strand, self.frame, self.attributes_str]
            ) + '\n'

    def __lt__(self, other):
        width = self.end - self.start
        other_width = other.end - other.start
        return width < other_width

    def __gt__(self, other):
        width = self.end - self.start
        other_width = other.end - other.start
        return width > other_width

    def identical_region(self, other):
        """
        Check if two features are identical
        :param other:
        :return:
        """
        if self.start == other.start and self.end == other.end and self.header == \
                other.header:
            return True
        else:
            return False



def read_fasta_sequence_size(fasta_file):
    """Read size of sequence into dictionary"""
    fasta_dict = {}
    with open(fasta_file, 'r') as f:
        for line in f:
            if line[0] == '>':
                # remove part of name after space or tab
                header = line.strip().split()[0][1:]
                fasta_dict[header] = 0
            else:
                fasta_dict[header] += len(line.strip())
    return fasta_dict

def read_single_fasta_to_dictionary(fh):
    """
    Read fasta file into dictionary
    :param fh:
    :return:
    fasta_dict
    """
    fasta_dict = {}
    for line in fh:
        if line[0] == '>':
            header = line.strip().split()[0][1:]  # remove part of name after space
            fasta_dict[header] = []
        else:
            fasta_dict[header] += [line.strip()]
    fasta_dict = {k: ''.join(v) for k, v in fasta_dict.items()}
    return fasta_dict

def split_fasta_to_parts(fasta_file, number_of_parts):
    fasta_file_dir = tempfile.mkdtemp()
    # split fasta file to parts and save them to temporary directory
    # first count number of sequences
    with open(fasta_file, 'r') as f:
        number_of_sequences = 0
        for line in f:
            if line[0] == '>':
                number_of_sequences += 1
    part_size = int(number_of_sequences / number_of_parts) + 1
    # adjust last part size to
    last_part_size = number_of_sequences - (number_of_parts - 1) * part_size
    part_sizes = [part_size] * (number_of_parts - 1) + [last_part_size]
    part_files = [os.path.join(fasta_file_dir, F'part_{i}.fasta') for i in range(number_of_parts)]
    for i, size in enumerate(part_sizes):
        fn = part_files[i]
        with open(fasta_file, 'r') as f, open(fn, 'w') as f_out:
            n = 0
            while n < size:
                line = f.readline()
                if line[0] == '>':
                    n += 1
                if n <= size:
                    f_out.write(line)
            # set file pointer one line back
            f.seek(f.tell() - len(line))
    return part_files




def split_fasta_to_chunks(fasta_file, chunk_size=100000000, overlap=100000):
    """
    Split fasta file to chunks, sequences longe than chuck size are split to overlaping
    peaces. If sequences are shorter, chunck with multiple sequences are created.
    :param fasta_file:

    :param fasta_file:
    :param chunk_size:
    :param overlap:
    :return:
    fasta_file_split
    matching_table
    """
    min_chunk_size = chunk_size * 2
    print("analyzing fasta file")
    fasta_dict = read_fasta_sequence_size(fasta_file)
    print("fasta file loaded")
    print("number of sequences: ", len(fasta_dict))
    # calculates ranges for splitting of fasta files and store them in list
    matching_table = []
    fasta_file_split = tempfile.NamedTemporaryFile(delete=False).name
    print("calculating ranges for splitting of fasta files")
    for header, size in fasta_dict.items():
        if size > min_chunk_size:
            number_of_chunks = int(size / chunk_size)
            adjusted_chunk_size = int(size / number_of_chunks)
            for i in range(number_of_chunks):
                start = i * adjusted_chunk_size
                end = ((i + 1) *
                       adjusted_chunk_size
                       + overlap) if i + 1 < number_of_chunks else size
                new_header = header + '_' + str(i)
                matching_table.append([header, i, start, end, new_header])
        else:
            new_header = header + '_0'
            matching_table.append([header, 0, 0, size, new_header])
    print("splitting fasta file")
    # read sequences from fasta files and split them to chunks according to matching table
    # open output and input files, use with statement to close files
    # TODO - avoid using dictionary - could be problem for large files!!
    fasta_dict = read_single_fasta_to_dictionary(open(fasta_file, 'r'))
    with open(fasta_file_split, 'w') as fh_out:
        for header in fasta_dict:
            print(header)
            matching_table_part = [x for x in matching_table if x[0] == header]
            for header2, i, start, end, new_header in matching_table_part:
                fh_out.write('>' + new_header + '\n')
                fh_out.write(fasta_dict[header][start:end] + '\n')
    return fasta_file_split, matching_table

def make_temp_files(number_of_files):
    """
    Make named temporary files, file will not be deleted upon exit!
    :param number_of_files:
    :return:
    filepaths
    """
    temp_files = []
    for i in range(number_of_files):
        temp_files.append(tempfile.NamedTemporaryFile(delete=False).name)
        os.remove(temp_files[-1])
    return temp_files


def get_new_header_and_coordinates(header, start, end, matching_table):
    """
    Get new header and coordinates for sequence
    :param header:
    :param start:
    :param end:
    :param matching_table:
    :return:
    new_header
    new_start
    new_end
    """
    matching_table_part = [x for x in matching_table if x[0] == header]
    new_coords = []
    for chunk in matching_table_part:
        if chunk[2] <= start < chunk[3]:
            new_header = chunk[4]
            new_start = start - chunk[2]
            new_end = end - chunk[2]
            new_sequence_length = chunk[3] - chunk[2]
            new_coords.append([new_header, new_start, new_end, new_sequence_length])
    return new_coords


def get_original_header_and_coordinates(new_header, new_start, new_end, matching_table):
    """
    Get original header and coordinates for sequence
    :param new_header:
    :param new_start:
    :param new_end:
    :param matching_table:
    :return:
    original_header
    original_start
    original_end
    """
    matching_table_part = [x for x in matching_table if x[4] == new_header]
    real_chunk_size = matching_table_part[0][3] - matching_table_part[0][2]
    ori_header = matching_table_part[0][0]
    start = matching_table_part[0][2]
    ori_start = new_start + start
    ori_end = new_end + start
    return ori_header, ori_start, ori_end, real_chunk_size


# recalculate gff3 coordinates, use gff3_feature class
def recalculate_gff3_coordinates(gff3_file, matching_table):
    """
    Recalculate gff3 coordinates, use gff3_feature class
    :param gff3_file:
    :param matching_table:
    :return:
    gff3_file_recalculated
    """
    gff3_file_recalculated = tempfile.NamedTemporaryFile(delete=False).name

    with open(gff3_file, 'r') as fh_in:
        with open(gff3_file_recalculated, 'w') as fh_out:
            for line in fh_in:
                if line[0] == '#':
                    fh_out.write(line)
                else:
                    feature = Gff3Feature(line)
                    new_coords = get_new_header_and_coordinates(
                        feature.header, feature.start, feature.end, matching_table
                        )
                    for new_header, new_start, new_end, sequence_length in new_coords:
                        if new_start >= 1 and new_end <= sequence_length:
                            feature.header = new_header
                            feature.start = new_start
                            feature.end = new_end
                            fh_out.write(str(feature))
    return gff3_file_recalculated


# recalculate gff3 back to original coordinates, use gff3_feature class
def recalculate_gff3_back_to_original_coordinates(gff3_file, matching_table,
                                                  chunk_size, overlap):
    """
    Recalculate gff3 back to original coordinates, use gff3_feature class
    :param gff3_file:
    :param matching_table:
    :return:
    gff3_file_recalculated
    """
    gff3_file_recalculated = tempfile.NamedTemporaryFile(delete=False).name
    with open(gff3_file, 'r') as fh_in:
        with open(gff3_file_recalculated, 'w') as fh_out:
            for line in fh_in:
                if line[0] == '#':
                    fh_out.write(line)
                else:
                    feature = Gff3Feature(line)
                    ori_header, ori_start, ori_end, \
                        real_chunk_size = get_original_header_and_coordinates(
                        feature.header, feature.start, feature.end, matching_table
                        )
                    # if feature is too close (less than 100 nt) to the end ends of
                    # chunk,skip it
                    if feature.start < 100 or feature.end > real_chunk_size - 100:
                        continue
                    feature.header = ori_header
                    feature.start = ori_start
                    feature.end = ori_end
                    fh_out.write(str(feature))
    return gff3_file_recalculated



class CustomFormatter(
    argparse.ArgumentDefaultsHelpFormatter, argparse.RawDescriptionHelpFormatter
    ):
    """This is custom formatter for argparse"""
    pass

def add_version_to_gff3(gff3_file, version_info, REXdb_version):
    """
    Add version to gff3 file
    :param gff3_file:
    :param version_info:
    :return:
    """
    tmp_filename = os.path.join(gff3_file + '.tmp')
    with open(gff3_file, 'r') as src_file, open(tmp_filename, 'w') as tmp_file:
        line = src_file.readline()
        tmp_file.write(line)
        tmp_file.write("##DANTE version " + version_info + '\n')
        tmp_file.write("##REXdb version " + REXdb_version + '\n')
        shutil.copyfileobj(src_file, tmp_file)
        # Replace the original file with the new file
    os.replace(tmp_file.name, gff3_file)



parser = argparse.ArgumentParser(
    description='''Script performs similarity search on given DNA sequence(s) in (
    multi)fasta against our protein domains database of all Transposable element for 
    certain group of organisms (Viridiplantae or Metazoans). Domains are subsequently 
    annotated and classified - in case certain domain has multiple annotations 
    assigned, classification is derived from the common classification level of all of 
    them. Domains search is accomplished engaging LASTAL alignment tool.



    ''', epilog="""
    Extra database format:
    Extra database is FASTA file with protein domains sequences. This file is appended 
    to selected REXdb database. Header of sequences must contain information about 
    classification compatible with REXdb classification system and also protein domain 
    type. Example of FASTA header:
    
       >MNCI01000001.1:152848-153282 RH Class_I|LTR|Ty3/gypsy|non-chromovirus|OTA|Tat
       CQEALDNIMRELAQVSTVYSSQNDKSFYIYLTISDisissllcQKLDDGVELsvyylsha
       litYET*YIEVEKFFLALVVSFKK*rnylfrshINVICKDKVLRDITTNIYKNSRIA**K
       DILDEFGfhyisqa*TKGQVIATQLT
    
    where:
    
    MNCI01000001.1:152848-153282 is unique identifier of sequence in database.
    
    RH is type of protein domain.
    
    Class_I|LTR|Ty3/gypsy|non-chromovirus|OTA|Tat is classification of protein domain.
    
    """, formatter_class=CustomFormatter
    )

requiredNamed = parser.add_argument_group('required named arguments')
requiredNamed.add_argument(
    "-q", "--query", type=str, required=True,
    help='input DNA sequence to search for protein domains in a fasta format. '
         'Multifasta format allowed.'
    )
parser.add_argument('-D', '--database', type=str, required=False,
                    default='Viridiplantae_v4.0',
                    choices=[
                        'Viridiplantae_v4.0',
                        'Viridiplantae_v3.0',
                        'Metazoa_v3.1',
                        'Viridiplantae_v2.2',
                        'Metazoa_v3.0'],
                    help='Select version of RExDB database to use for similarity search'
                    )


parser.add_argument(
    "-o", "--domain_gff", type=str, help="output domains gff format",required=True
    )
parser.add_argument(
    "-dir", "--output_dir", type=str,
    help="specify if you want to change the output directory",
    default='.'
    )
parser.add_argument(
    "-M", "--scoring_matrix", type=str, default="BL80", choices=['BL80', 'BL62', 'MIQS'],
    help="specify scoring matrix to use for similarity search (BL80, BL62, MIQS)"
    )
parser.add_argument(
    "-thsc", "--threshold_score", type=int, default=80,
    help="percentage of the best score in the cluster to be tolerated when assigning "
         "annotations per base"
    )
parser.add_argument(
    "-wd", "--win_dom", type=int, default=10000000,
    help="window to process large input sequences sequentially"
    )
parser.add_argument(
    "-od", "--overlap_dom", type=int, default=10000,
    help="overlap of sequences in two consecutive windows"
    )
parser.add_argument(
    "-c", "--cpu", type=int, default=1,
    help="number of threads to use"
    )
parser.add_argument('-S', '--short_reads', action='store_true',
                    help='use this option if your input sequences are short reads',
                    default=False
                    )


parser.add_argument(
        '-e', '--extra_database', type=str, required=False,
        default=None,help='extra database to use for similarity search')

parser.add_argument('-v', '--version', action='version', version='%(prog)s ' + __version__)


args = parser.parse_args()
print(args)
script_path = os.path.dirname(os.path.realpath(__file__))
# add path to database files

pdb = F'{script_path}/tool-data/protein_domains/{args.database}_pdb'
cls = F'{script_path}/tool-data/protein_domains/{args.database}_class'


if args.extra_database is not None:
    # add extra database to default database
    tmpdir_obj = tempfile.TemporaryDirectory()
    db_tmp_dir = tmpdir_obj.name
    os.makedirs(db_tmp_dir, exist_ok=True)
    # TODO - unlink directory at the end
    classification_table = set()
    extra_classification_file = os.path.join(db_tmp_dir, 'extra.class')
    extra_fasta_file = os.path.join(db_tmp_dir, 'extra.fasta')
    with open(args.extra_database, 'r') as f, open(extra_fasta_file, 'w') as out:
        for line in f:
            if line[0] == ">":
                ## modify header
                name, domain, classification = line.split(" ")
                name_clean=name[1:].replace("-","_")
                new_header = ">NA-{}__{}\n".format(domain, name_clean)
                classification_string = "\t".join(classification.split("|"))
                classification_table.add("{}\t{}".format(name_clean, classification_string))
                out.write(new_header)
            else:
                out.write(line)
    with open(extra_classification_file, 'w') as f:
        f.writelines(classification_table)
    # concatenate extra database with pdb and cls to temporary files
    pdb_tmp = os.path.join(db_tmp_dir, 'pdb.fasta')
    cls_tmp = os.path.join(db_tmp_dir, 'cls.class')
    # concatenate files
    with open(pdb_tmp, 'w') as f:
        for file in [pdb, extra_fasta_file]:
            with open(file, 'r') as f_tmp:
                for line in f_tmp:
                    f.write(line)
    with open(cls_tmp, 'w') as f:
        for file in [cls, extra_classification_file]:
            with open(file, 'r') as f_tmp:
                for line in f_tmp:
                    f.write(line)
    # format lastdb
    cmd = ['lastdb', '-p', pdb_tmp,  pdb_tmp]
    subprocess.run(cmd)
    pdb = pdb_tmp
    cls = cls_tmp


args.protein_database = pdb
args.classification = cls
args.new_ldb = False
# run dante
chunk_size = 500000
overlap = 2000


if not args.short_reads:
    # split fasta file to chunks
    fasta_file_split, matching_table = split_fasta_to_chunks(args.query, chunk_size, overlap)
    print('getting fasta sequence size')
    fasta_seq_size = read_fasta_sequence_size(fasta_file_split)
    total_size = sum(fasta_seq_size.values())
    number_of_sequences = len(fasta_seq_size)
    print('total size: ', total_size)
    print('number of sequences: ', number_of_sequences)
    print('splitting fasta file to chunks')

    if total_size > chunk_size and number_of_sequences > 1 and args.cpu > 1:
        print("running DANTE on chunks")
        seq_id_size_sorted = [i[0] for i in sorted(
            fasta_seq_size.items(), key=lambda x: int(x[1]), reverse=True
            )]
        number_of_temp_files = int(total_size / chunk_size) + 1
        if number_of_temp_files > number_of_sequences:
            number_of_temp_files = number_of_sequences

        temp_files_fasta = make_temp_files(number_of_temp_files)
        # do not used file handles as it can cause oproblem with maximum number of open files
        seq_id_file_path_dict = dict(zip(seq_id_size_sorted, cycle(temp_files_fasta)))
        # write sequences to temporary files
        with open(fasta_file_split, 'r') as f:
            for line in f:
                if line[0] == '>':
                    header = line.strip().split(' ')[0][1:]
                    # append to file
                    with open(seq_id_file_path_dict[header], 'a') as fout:
                        fout.write(line)
                else:
                    with open(seq_id_file_path_dict[header], 'a') as fout:
                        fout.write(line)
        os.remove(fasta_file_split)
        args_list = []
        output_files = make_temp_files(number_of_temp_files)
        for i, temp_file_out in enumerate(temp_files_fasta):
            args_part = copy.deepcopy(args)
            args_list.append(args_part)
            args_part.query = temp_files_fasta[i]
            args_part.domain_gff = output_files[i]
            # dante.main(args_part)

        # run dante in parallel, use Pool after all runs finishes concatenate results,
        # use n_cpu and number of processes

        print("Number of chunks: ", number_of_temp_files)
        with Pool(args.cpu) as p:
            p.map(dante.main, args_list, chunksize=1)

        for fn in temp_files_fasta:
            os.remove(fn)


        # recalculate coordinates
        tmp_gff_unfiltered = tempfile.NamedTemporaryFile(delete=False).name
        with open(tmp_gff_unfiltered, 'w') as f:
            for args_part in args_list:
                tmp_gff=recalculate_gff3_back_to_original_coordinates(args_part.domain_gff,
                                                                     matching_table,
                                                                     chunk_size,
                                                                     overlap)
                with open(tmp_gff, 'r') as f_tmp:
                    for line in f_tmp:
                        f.write(line)
                os.remove(tmp_gff)
        # filter gff to remove duplicates (these are generated when overlapping chunks are
        # processed)
        gff3_header = "##gff-version 3\n"

        with open(args.domain_gff, 'w') as f, open(tmp_gff_unfiltered, 'r') as f_tmp:
            # sort gff, use sort command and read output from stdout
            p = subprocess.Popen(['sort', '-k1,1', '-k4,4n'],stdin=f_tmp,
                                 stdout=subprocess.PIPE)
            last_line_coord = ""
            f.write(gff3_header)
            for line in p.stdout:
                if line.decode('utf-8').startswith('#'):
                    continue
                items=line.decode('utf-8').split('\t')
                coord = F'{items[0]}_{items[3]}_{items[4]}'
                if coord != last_line_coord:
                    f.write(line.decode('utf-8'))
                    last_line_coord = coord
            for line in f_tmp:
                f.write(line)
        # clean up
        os.remove(tmp_gff_unfiltered)
        # remove temporary files
        for args_part in args_list:
            os.remove(args_part.domain_gff)
    else:
        print("running DANTE on whole file")
        dante.main(args)
else:
    if args.cpu==1:
        print("running DANTE on short reads with 1 cpu")
        dante.main(args)
    else:
        # simply split input file to args.cpu pieces and run in parallel
        print("Spliting input file to parts")
        parts = split_fasta_to_parts(args.query, number_of_parts=args.cpu)
        args_list = []
        output_files = make_temp_files(args.cpu)
        for i, part in enumerate(parts):
            args_part = copy.deepcopy(args)
            args_list.append(args_part)
            args_part.query = part
            args_part.domain_gff = output_files[i]
        with Pool(args.cpu) as p:
            p.map(dante.main, args_list)
        # concatenate results
        with open(args.domain_gff, 'w') as f:
            for output_file in output_files:
                with open(output_file, 'r') as f_tmp:
                    for line in f_tmp:
                        f.write(line)
                os.remove(output_file)
        # clean up - delete whole directory and its content
        shutil.rmtree(os.path.dirname(parts[0]))






# Add version to gff3 file
add_version_to_gff3(args.domain_gff, __version__, args.database)
# remove temporary files, recursive remove directory
if args.extra_database is not None:
    shutil.rmtree(db_tmp_dir)

shutil.rmtree(master_tmp)
